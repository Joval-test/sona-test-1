{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://ai-gpu-ps.openai.azure.com/\",\n",
    "        azure_deployment=\"gpt40-mini-long-context\",\n",
    "        openai_api_version=\"2024-05-01-preview\",\n",
    "        api_key=\"dc415207c54e4dd8ba8b60cb66374822\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_collection = Chroma(\n",
    "        collection_name=\"company_info_store\",\n",
    "        persist_directory=\"chroma_storage\",\n",
    "        embedding_function=embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash ID 'fa510296a7d556be18bf44d452698dc12c44e06edd0de9d3d5a2f81a5ca811f9' does not exist in the collection.\n"
     ]
    }
   ],
   "source": [
    "hash_id = \"fa510296a7d556be18bf44d452698dc12c44e06edd0de9d3d5a2f81a5ca811f9\"\n",
    "\n",
    "all_ids = company_collection.get()[\"ids\"]\n",
    "if hash_id in all_ids:\n",
    "    print(f\"Hash ID '{hash_id}' exists in the collection.\")\n",
    "else:\n",
    "    print(f\"Hash ID '{hash_id}' does not exist in the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_docs = company_collection.get(where={\"content_hash\": content_hash})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3.1\",base_url=\"3.110.55.125:11434\")\n",
    "response=llm.invoke(\"hai who is this?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I want to create a simple chatbot using the DeepSeek R1 model in Python. I remember seeing some examples online, but I'm not exactly sure where to start. Let's see... First, I need to figure out which library supports interacting with the DeepSeek model.\n",
      "\n",
      "I've heard of something called TenT and OpenAI has an SDK. Maybe I can use one of those? Wait, maybe there's a Python client specifically for DeepSeek R1. I think I saw something about it on Hugging Face or another platform. I should probably check if there's a Python package available that wraps the DeepSeek API.\n",
      "\n",
      "I recall that sometimes models are accessible through Restful APIs, so perhaps I can make HTTP requests from my Python code to an endpoint provided by DeepSeek. But then I might need some authentication, which could complicate things. Alternatively, maybe there's a more convenient way using existing libraries like tenT or something else.\n",
      "\n",
      "Wait, the user mentioned TenT in their initial answer. So maybe using the `deepseek` library via pip would be the right approach. Let me check if that's available on PyPI. I think it is because they provided a link to the GitHub repository and documentation. Great, so installing it with pip should work.\n",
      "\n",
      "Now, setting up the environment variables. The user mentioned something about .env files for API keys. That makes sense—without proper authentication, the model won't respond correctly. So I need to create a .env file where I store my DeepSeek API key and the model parameters like temperature and max tokens.\n",
      "\n",
      "I'm not entirely sure how the request library works in this context. In their code snippet, they used requests.get with an options object that includes headers and params. The params include 'temperature' and 'max_tokens', which control the response style and length. I should probably set these to appropriate values for my use case.\n",
      "\n",
      "The response from the model is JSON, so parsing it with json.loads makes sense. Then extracting the content from the result using .get('choices') and [0]['message']['content'] seems straightforward. But wait, what if the API returns an error? I should handle exceptions or make sure to test the code thoroughly.\n",
      "\n",
      "Also, considering that some APIs have rate limits, maybe adding a delay between requests could be useful. The user's example has a 1-second delay after each get request, which might help prevent hitting rate limits. But I'm not sure how many tokens are allowed per minute—maybe checking the model's API documentation would clarify.\n",
      "\n",
      "Putting it all together, the code structure looks logical: importing necessary modules, setting up environment variables, defining functions to make API calls and format responses, initializing the client with the model name and parameters, handling user input in a loop, prompting for exit, and adding basic error handling.\n",
      "\n",
      "I'm a bit concerned about security—storing API keys in a .env file. Maybe I should use a more secure method like environment variables in a Docker container or a service like AWS Lambda. But that might be beyond the scope of my current project; perhaps starting with the simple setup is better for now.\n",
      "\n",
      "Testing each part step by step would help catch any issues early. For example, ensuring the .env file is correctly created and the API key is there. Also, checking if the requests library works without any errors when making a GET request to the specified endpoint.\n",
      "\n",
      "Another thought: maybe using streaming could be more efficient for large responses or long-running interactions. But the user's code doesn't implement that, so perhaps it's better to stick with standard HTTP requests for simplicity until I need more advanced features.\n",
      "\n",
      "I should also consider whether there are any specific headers required beyond the Content-Type and Authorization. The example uses 'application/json' as the content type, which seems standard for JSON responses.\n",
      "\n",
      "In summary, following the provided code outline would involve installing dependencies, setting up environment variables, defining helper functions, initializing the client with model parameters, handling user input in a loop, and properly formatting and displaying responses. Testing each component separately before integrating them into the main function is crucial to ensure everything works smoothly.\n",
      "</think>\n",
      "\n",
      "To create a simple chatbot using the DeepSeek R1 model in Python, follow these steps:\n",
      "\n",
      "### Step 1: Install Necessary Packages\n",
      "First, install the required Python packages. Use pip to install the `deepseek` library and any other dependencies.\n",
      "\n",
      "```bash\n",
      "pip install deepseek requests\n",
      "```\n",
      "\n",
      "### Step 2: Create Your Environment Variables\n",
      "Set up your environment variables with the necessary API keys and model parameters. For example:\n",
      "\n",
      "Create a `.env` file in your project root:\n",
      "\n",
      "```text\n",
      "<<<<<<< SEARCH\n",
      "=======\n",
      "DEEPSEEK_API_KEY=your_api_key_here\n",
      "DEEPSEEK_MODEL=R1-en vigyapi-stable\n",
      ">>>>>>> REPLACE\n",
      "```\n",
      "\n",
      "Replace `your_api_key_here` with your actual DeepSeek API key.\n",
      "\n",
      "### Step 3: Implement the Chatbot Code\n",
      "\n",
      "Here's a basic implementation of a chatbot using the DeepSeek R1 model:\n",
      "\n",
      "```python\n",
      "from deepseek import DeepSeek\n",
      "import os\n",
      "import json\n",
      "import requests\n",
      "from time import sleep\n",
      "\n",
      "# Initialize DeepSeek client with your API key and model parameters\n",
      "client = DeepSeek(\n",
      "    api_key=os.getenv('DEEPSEEK_API_KEY'),\n",
      "    model='R1-en-vigyapi-stable',\n",
      "    temperature=0.7,\n",
      "    max_tokens=2000,\n",
      "    n_ctx=4096\n",
      ")\n",
      "\n",
      "def format_response(response):\n",
      "    \"\"\"Format the model's response.\"\"\"\n",
      "    return response.get('choices', []) \\\n",
      "           .get('message', []) \\\n",
      "           .get('content', '')\n",
      "    \n",
      "def ask_chatbot(user_input):\n",
      "    \"\"\"Handle user input and return a formatted response.\"\"\"\n",
      "    try:\n",
      "        # Make API request\n",
      "        response = requests.get(\n",
      "            'https://api.deepseek.com/v1/chat/completions',\n",
      "            headers={'Content-Type': 'application/json'},\n",
      "            json={\n",
      "                'model': client.model,\n",
      "                'messages': [{\n",
      "                    'role': 'user',\n",
      "                    'content': user_input\n",
      "                }]\n",
      "            },\n",
      "            params={\n",
      "                'temperature': str(client.temperature),\n",
      "                'max_tokens': str(client.max_tokens),\n",
      "                'n_ctx': str(client.n_ctx)\n",
      "            }\n",
      "        )\n",
      "        \n",
      "        # Check for errors\n",
      "        if response.status_code != 200:\n",
      "            raise ValueError(f\"API request failed with status code {response.status_code}\")\n",
      "            \n",
      "        return format_response(response.json())\n",
      "        \n",
      "    except requests.exceptions.RequestException as e:\n",
      "        raise ValueError(f\"Request to DeepSeek API failed: {str(e)}\")\n",
      "    \n",
      "    # Add a small delay between requests\n",
      "    sleep(1)\n",
      "\n",
      "def main():\n",
      "    print(\"Chatbot initialized. Type 'exit' to quit.\")\n",
      "    while True:\n",
      "        user_input = input(\"You: \").strip()\n",
      "        if not user_input:\n",
      "            continue\n",
      "        \n",
      "        response = ask_chatbot(user_input)\n",
      "        \n",
      "        # Display the response in a user-friendly format\n",
      "        print(\"\\nAI:\", response)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. **Initialization**: The `DeepSeek` client is initialized with your API key and model parameters (temperature, max_tokens, n_ctx).\n",
      "\n",
      "2. **Response Formatting**: The `format_response` function extracts the user's response from the JSON payload returned by the DeepSeek API.\n",
      "\n",
      "3. **Asking the Chatbot**: The `ask_chatbot` function sends a request to the DeepSeek API endpoint with your user input as part of a conversation prompt. It handles errors and returns formatted responses.\n",
      "\n",
      "4. **Main Loop**: The `main` function runs an infinite loop where it prompts the user, calls the chatbot, and displays the response until the user types \"exit\".\n",
      "\n",
      "### Notes\n",
      "\n",
      "- **API Key**: Ensure you have a valid DeepSeek API key and replace it in your `.env` file.\n",
      "- **Model Parameters**: Adjust `temperature`, `max_tokens`, and `n_ctx` based on your needs. Lower temperature values (0 to 1) make responses more focused, while higher values can lead to more creative outputs.\n",
      "- **Rate Limits**: Be aware of API rate limits and consider adding appropriate delays between requests.\n",
      "\n",
      "This code provides a basic structure for a chatbot that you can expand with additional features or integrate into a larger application.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"deepseek-r1\", base_url=\"43.205.112.102:11434\")\n",
    "\n",
    "response=llm.invoke(\"Okay i'm building an app with deepseek r1 model in my python code. its a chat bot so can u give me how to do that with the help of  basic python code?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "context_instr = '''\n",
    "<< YOUR TASK >>\n",
    "You are an AI assistant designed to contact potential business prospects via chat or phone. Your purpose is to identify if the prospect is experiencing specific problems we solve, showcase the value of our solutions, and guide them toward validating and closing the interaction (e.g., scheduling a meeting or exchanging contact details). Follow these guidelines:\n",
    "\n",
    "1. First Message Format:\n",
    "- Use the user information to personalize your greeting\n",
    "- Format: \"Hi [name]! I'm an AI assistant from [company]. Do you have a few minutes to chat?\"\n",
    "- If no name is found, use the above greeting without the name parameter to make it generic\n",
    "- From the company info try to find in the data << COMPANY INFO >>\n",
    "\n",
    "\n",
    "2. Identify if the prospect is experiencing known problems:\n",
    " Utilise data in << USER INFO >> to infer which one of the company's solutions would be most relevant to the user.\n",
    "Examples for doing this naturally:\n",
    "\"I saw (something the user is doing/working on/position of user), which led me to believe you're trying to (whatever problem you solve). Are you?\"\n",
    "\"I saw that you were running Google ads for laser tattoo removal. That led me to believe you might be looking to bring in more patients. Is that a priority for you right now?\"\n",
    "\"Many small business owners spend a few hours every week using tools like PayPal, Stripe, and spreadsheets to manually do their bookkeeping. I'm curious. How are you handling bookkeeping at ABC Company?\"\n",
    "\n",
    "If there is nothing in << USER INFO >> that would give you a hint as to which solution of the company would be relevant to the user, identify this at the earliest.\n",
    "Present the top 3 common problems we solve.\n",
    "General example to do this:\n",
    "\"Most (roles) that I talk to tell me that they're trying to do one of three things: (problem 1), (problem 2), or (problem 3). Does that sound like you at all?\"\n",
    "Specific example:\n",
    "\"Most of the doctors I talk to tell me they are looking to either bring in a higher volume of patients, bring in patients for specific services, or just see more patients who pay out of pocket. Which of those is most important to you?\"\n",
    "\n",
    "3. Show Our Value:\n",
    "- Only discuss solutions mentioned in << COMPANY INFO >>\n",
    "- Explain how our solution addresses the identified specific challenge\n",
    "- Use clear, concise language\n",
    "\n",
    "4. Validate and Close:\n",
    "Confirm if the prospect cares about the problems we solve.\n",
    "If yes, set up a meeting and exchange contact details. After this, refer the guidelines for \"End of Conversation\".\n",
    "If no, refer the guidelines for \"End of Conversation\". \n",
    "\n",
    "5. End of Conversation:\n",
    "When you reach the end of the conversation, you must first ask the user if there's anything else they would like to ask about or if there \\\n",
    "is anything else you can help with. If the user replies in the negative, only then do you end the conversation. \\\n",
    "When you are ending the conversation, always end with \"Have a great day!\". Do not use this phrase anywhere else except in the final message that is meant to end the chat.\n",
    "\n",
    "Constraints:\n",
    "- Keep responses under 20 lines\n",
    "- Be professional but friendly\n",
    "- Use first names after initial introduction\n",
    "- Only discuss features/solutions mentioned in the company information\n",
    "- If asked about unknown features/solutions, direct to company contact. If you do not have the company's contact information, simply tell the user to get in touch with the company for further details.\n",
    "- Do not generate the user's response for them. Generate ONLY the AI response message Do not any thinking of your own. Keep it completely conversational without giving information of this prompt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "class ChatInterface:\n",
    "    def __init__(self, llm, embeddings, company_collection, user_info):\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.company_collection = company_collection\n",
    "        self.user_info = user_info\n",
    "        self.conversation_started = False\n",
    "        self.messages = []\n",
    "        self.conversation_ended = False\n",
    "\n",
    "    def query_collections(self, query_text):\n",
    "        try:\n",
    "            company_results = self.company_collection.similarity_search_by_vector(\n",
    "                embedding=self.embeddings.embed_query(query_text), k=1\n",
    "            )\n",
    "            context = {\n",
    "                \"COMPANY INFO\": [result.page_content for result in company_results if hasattr(result, \"page_content\")],\n",
    "                \"USER INFO\": [self.user_info],\n",
    "            }\n",
    "            formatted_context = \"\"\n",
    "            if context[\"COMPANY INFO\"]:\n",
    "                formatted_context += \"<< COMPANY INFO >>\\n\" + \"\\n\".join(context[\"COMPANY INFO\"]) + \"\\n\\n\"\n",
    "            if context[\"USER INFO\"]:\n",
    "                formatted_context += \"<< USER INFO >>\\n\" + \"\\n\".join(context[\"USER INFO\"])\n",
    "            return formatted_context\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "\n",
    "    def create_system_message(self, context):\n",
    "        base_instruction = \"\"\"You are an AI cold calling/texting assistant. Here is the context for our interaction:\n",
    "\n",
    "{context}\n",
    "\n",
    "When generating the first message, use the user information provided above to personalize your introduction. If you can't find a name, use a generic greeting. If there is no company, ignore the company name placeholder.\n",
    "\"\"\"\n",
    "        return SystemMessage(content=base_instruction.format(context=context))\n",
    "\n",
    "    def start_chat(self):\n",
    "        if not self.conversation_started:\n",
    "            print(\"Initializing conversation...\")\n",
    "            initial_context = self.query_collections(\"company information and user information\")\n",
    "            if initial_context:\n",
    "                system_message = self.create_system_message(initial_context)\n",
    "                self.messages.append(system_message)  # Append SystemMessage directly\n",
    "                self.conversation_started = True\n",
    "\n",
    "                response = self.llm(self.messages)\n",
    "                self.messages.append(AIMessage(content=response.content))  # Append AIMessage for responses\n",
    "                print(f\"Assistant: {response.content}\")\n",
    "            else:\n",
    "                print(\"Error: No context found. Please ensure company and user information is properly processed.\")\n",
    "                return\n",
    "\n",
    "        while not self.conversation_ended:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                self.conversation_ended = True\n",
    "                print(\"Conversation ended. Have a great day!\")\n",
    "                return\n",
    "\n",
    "            self.messages.append(HumanMessage(content=user_input))\n",
    "            context = self.query_collections(user_input)\n",
    "            self.messages[0] = self.create_system_message(context)  # Update system message\n",
    "\n",
    "            response = self.llm(self.messages)\n",
    "            self.messages.append(AIMessage(content=response.content))\n",
    "            print(f\"Assistant: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing conversation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24316\\459873461.py:49: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.llm(self.messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \n",
      "Assistant: Hi Akash, this is Rohan from TechHive calling about a Front-End Developer position that we think might be a great fit for you. We're looking for someone with 2+ years of experience in front-end development to join our team in Bengaluru. Would you like to know more about the role and its responsibilities?\n",
      "Assistant: We'd love to have you on board, Akash! The position involves developing user-facing features for web applications, collaborating with designers and backend developers, and optimizing applications for performance and scalability. We're looking for someone with strong problem-solving skills and attention to detail.\n",
      "\n",
      "Can you tell me a bit about your current experience as a Backend Developer? What makes you interested in switching to Front-End Development?\n",
      "Assistant: So, Akash, it sounds like you have some experience in backend development, but are looking to make the switch to frontend. We'd love to discuss how your skills can be applied to this role.\n",
      "\n",
      "Can you tell me a bit more about your technical skills? Do you have any experience with HTML, CSS, JavaScript, or front-end frameworks like React, Angular, or Vue.js?\n",
      "Assistant: Having a solid foundation in HTML and CSS is definitely a great starting point for frontend development. We're looking for someone who can write efficient and maintainable code, so it sounds like you might be a good fit.\n",
      "\n",
      "Can I schedule a time to speak with our team lead about the role and answer any questions you may have? It would also give us an opportunity to discuss how we can support your transition from backend to frontend development.\n",
      "Assistant: I'll send over some dates and times for you to choose from, Akash. In the meantime, I'll also send over a link to our company website so you can learn more about us and the role.\n",
      "\n",
      "Would you prefer a call or a meeting in person? And would any of the following times work for you: tomorrow at 2 PM, Wednesday at 10 AM, or Thursday at 3 PM?\n",
      "Assistant: I'll send over an email with all the details to akash@cazelabs.com (just a heads up, I used your company email instead of the one you provided). The email will include the meeting invite, company website link, and any other relevant information.\n",
      "\n",
      "Looking forward to speaking with you soon, Akash!\n",
      "Assistant: You're welcome, Akash. Have a great day and I'll talk to you soon. Bye for now!\n",
      "Assistant: (Email sent)\n",
      "\n",
      "Subject: Front-End Developer Position at TechHive\n",
      "\n",
      "Dear Akash,\n",
      "\n",
      "I hope this email finds you well. We previously discussed the Front-End Developer position at TechHive, and we're excited about the possibility of having you join our team.\n",
      "\n",
      "Below are the meeting invite details:\n",
      "\n",
      "* Date: Tomorrow\n",
      "* Time: 2 PM IST\n",
      "* Location: Bengaluru Office\n",
      "\n",
      "You can also learn more about us and the role by visiting our company website: [www.techhive.com](http://www.techhive.com)\n",
      "\n",
      "If you have any questions or need further information, please don't hesitate to reach out.\n",
      "\n",
      "Looking forward to speaking with you soon!\n",
      "\n",
      "Best regards,\n",
      "Rohan\n",
      "TechHive Recruitment Team\n",
      "Assistant: (Email sent)\n",
      "\n",
      "Subject: Front-End Developer Position at TechHive - Follow-up\n",
      "\n",
      "Dear Akash,\n",
      "\n",
      "Just a quick follow-up on the email I sent earlier. If you have any questions or need more information about the Front-End Developer position, please don't hesitate to reach out.\n",
      "\n",
      "Also, if you're unable to make it to the meeting tomorrow, please let me know as soon as possible so we can schedule an alternative time that suits you.\n",
      "\n",
      "Looking forward to hearing back from you!\n",
      "\n",
      "Best regards,\n",
      "Rohan\n",
      "TechHive Recruitment Team\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     16\u001b[0m user_info\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mID: 3\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124mName: Akash\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124mCompany: Caze labs\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124mSource: User_leads (1).xlsx\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124mConnected: Yes\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     25\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatInterface(llm, embeddings, company_collection, user_info)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mChatInterface.start_chat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_ended:\n\u001b[1;32m---> 57\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\cold_chatbot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\cold_chatbot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(\n",
    "        model=\"llama3.1\",\n",
    "        base_url=\"43.205.112.102:11434\",\n",
    "        temperature=0.1)\n",
    "        \n",
    "    \n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\",base_url=\"43.205.112.102:11434\")\n",
    "\n",
    "company_collection = company_collection = Chroma(\n",
    "        collection_name=\"company_info_store\",\n",
    "        persist_directory=f\"chroma_storage\",\n",
    "        embedding_function=embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "user_info= \"\"\"ID: 3\n",
    "Name: Akash\n",
    "Company: Caze labs\n",
    "Phone Number: 0\n",
    "Age: 19\n",
    "Description: Backened developer waiting to switch to frontend\n",
    "Source: User_leads (1).xlsx\n",
    "Connected: Yes\"\"\"\n",
    "\n",
    "chat = ChatInterface(llm, embeddings, company_collection, user_info)\n",
    "chat.start_chat()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cold_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
